<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reproducible Research on Stephen J Eglen</title>
    <link>http://sje30.github.io/categories/reproducible-research/index.xml</link>
    <description>Recent content in Reproducible Research on Stephen J Eglen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>2016 ~ Stephen J Eglen</copyright>
    <atom:link href="/categories/reproducible-research/index.xml" rel="self" type="application/rss+xml" />
    
      
        
          <item>
            <title>A brief introduction to reproducible research and open science</title>
            <link>http://sje30.github.io/post/brief-open/</link>
            <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
            
            <guid>http://sje30.github.io/post/brief-open/</guid>
            <description>&lt;p&gt;[The following was written for the &lt;a href=&#34;http://www.bna.org.uk&#34;&gt;British Neuroscience Association&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;A brief introduction to reproducible research and open science&lt;/p&gt;

&lt;p&gt;At first glance, it might seem odd that you would need to prefix the
term &amp;ldquo;research&amp;rdquo; with the qualifier &amp;ldquo;reproducible&amp;rdquo;.  Surely, once you
have a paper in your hands, you have all the details in front of you
to reproduce someone else&amp;rsquo;s work?  That&amp;rsquo;s certainly the theory when
writing the paper, but it&amp;rsquo;s often not the practice&amp;hellip;  Since 2004
we&amp;rsquo;ve set a problem for our masters students to reproduce key results
from a paper within computational biology.  Even though students
carefully select a paper where the methods section seems
comprehensive, and all the experimental data are available, they
invariably find that many details are missing that preclude them from
reproducing key figures or results.  Many papers have been published
on this failure to reproduce [e.g. 2], commonly termed the
&amp;ldquo;reproducibility crisis&amp;rdquo; [1].&lt;/p&gt;

&lt;p&gt;So, what might reproducible research entail?  The definition can vary
from group to group, but my interpretation is that when publishing
result, labs should also provide all relevant datasets and methodology
for transforming data into results.  This means providing the
spreadsheets or computational scripts to reproduce analysis.  In turn,
this means that researchers should move away from &amp;ldquo;point and click&amp;rdquo;
analysis methodologies (doing a t-test in Excel) towards computer
scripts (such as R, matlab or python) so that others can re-run the
same routines.&lt;/p&gt;

&lt;p&gt;So, this brings us naturally to the second term, open science.  The
competitive nature (for limited funding, jobs, and &amp;ldquo;high impact&amp;rdquo;
publications) of science means that there is a natural tendency to
withold key datasets or analysis technologies: why give away your
results to your competitors?  An alternative view gaining prominence
in recent years is that by sharing our resources, we allow others to
build on our work and science as a whole should benefit.  By being an
open scientist, there are increased chances of making your work
reproducible.&lt;/p&gt;

&lt;p&gt;Being an open scientist may seem naïve and altruistic, but there are
selfish reasons for sharing your research [3].  Many
funding agencies now require data management plans for sharing of data
post publication, and journals are increasingly asking for data and
methods.  My optimistic hope is that in 10 years we might be able to
drop the qualifier &amp;ldquo;open&amp;rdquo; and instead talk again simply about science.&lt;/p&gt;

&lt;p&gt;Top tips for becoming an open scientist.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read the guidelines in [3] and think if they would apply to you.&lt;/li&gt;
&lt;li&gt;Read about experiences such as &lt;a href=&#34;https://www.theguardian.com/higher-education-network/blog/2014/aug/22/university-research-publish-open-access-journal&#34;&gt;Erin McKiernan&lt;/a&gt;.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Do experiments?  Try writing a registered report before doing the
experiments to reduce publication bias.
&lt;a href=&#34;https://www.nature.com/articles/s41562-016-0034&#34;&gt;https://www.nature.com/articles/s41562-016-0034&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Talk to your local library to see what services they can offer to
help archive and share your research.  Find a local community of
like-minded scientists!&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Learn how to code, rather than using Excel, for your data
analysis.  e.g. www.datacarpentry.org&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Comments?  Send them to me twitter @StephenEglen&lt;/p&gt;

&lt;p&gt;References&lt;/p&gt;

&lt;p&gt;[1] Baker M (2016) 1,500 scientists lift the lid on
reproducibility. Nature 533:452–454.  &lt;a href=&#34;http://dx.doi.org/10.1038/533452a&#34;&gt;http://dx.doi.org/10.1038/533452a&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] Ioannidis JPA (2005) Why most published research findings are
false. PLoS Med 2:e124. &lt;a href=&#34;https://doi.org/10.1371/journal.pmed.0020124&#34;&gt;https://doi.org/10.1371/journal.pmed.0020124&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] Markowetz F (2015) Five selfish reasons to work reproducibly. Genome
Biol 16:274.
&lt;a href=&#34;https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0850-7&#34;&gt;https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0850-7&lt;/a&gt;&lt;/p&gt;
</description>
          </item>
        
      
    
      
        
          <item>
            <title>An interactive introduction to computational neuroscience</title>
            <link>http://sje30.github.io/post/neuro-binder/</link>
            <pubDate>Thu, 16 Nov 2017 00:00:00 +0000</pubDate>
            
            <guid>http://sje30.github.io/post/neuro-binder/</guid>
            <description>&lt;p&gt;For a few years I&amp;rsquo;ve given a workshop, a two hour introduction to
computational neuroscience, for masters students.  This has covered
the Hodgkin-Huxley model of action potential generation, reduced
models, and simple small-networks.  Most of the content was previously
matlab code, provided by other researchers, most notably &lt;a href=&#34;http://cvr.yorku.ca/webpages/wilson.htm&#34;&gt;Hugh
Wilson&lt;/a&gt; and &lt;a href=&#34;https://www.izhikevich.org/&#34;&gt;Eugene
Izhikevich&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Over the last few months, we (mostly Derek Fulton and Paraskevi
Mylona) have converted this matlab code to the
&lt;a href=&#34;http://julialang.org&#34;&gt;Julia&lt;/a&gt; programming language.  This was partly
to evaluate Julia as a replacement for matlab (and the signs so far
are very encouraging) and also to evaluate recent cloud-based
infrastructure for running notebooks.  At the eleventh hour before
this year&amp;rsquo;s workshop I decided to switch to Julia for running the
workshop.  Thankfully, on the day all the technology worked perfectly.&lt;/p&gt;

&lt;p&gt;To try the workshop for yourself, simply visit the following link:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/sje30/julia-python/master?filepath=Introduction%20to%20Computational%20Neuroscience.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge.svg&#34; alt=&#34;Binder&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This should launch an interactive Jupyter notebook, where all the
graphs can be dynamically regenerated.  To try this, first go to the
&lt;em&gt;Cell menu -&amp;gt; All Output -&amp;gt; Clear&lt;/em&gt; to clear al the items and then
&lt;em&gt;Cell -&amp;gt; Run All&lt;/em&gt; to re-generate them.  (The first time you regenrate,
it may take a while as it needs to load various packages.)&lt;/p&gt;

&lt;p&gt;I think this technology is a great way of providing resources.
Students do not need to load any software onto their local machine;
all you need is a modern web browser and an internet connection.  No
more matlab licenses required! It also introduces students to the
notion of reproducible documents: live figures that can be
regenerated, rather than static figures.&lt;/p&gt;

&lt;p&gt;The tutorial is a bit rough around the edges, and not really
appropriate for independent study, but I intend to develop more
resources in this direction, and feedback welcome!&lt;/p&gt;
</description>
          </item>
        
      
    
      
        
          <item>
            <title>Towards standard practice for sharing of code and data in neuroscience</title>
            <link>http://sje30.github.io/post/code-sharing/</link>
            <pubDate>Thu, 25 May 2017 00:00:00 +0000</pubDate>
            
            <guid>http://sje30.github.io/post/code-sharing/</guid>
            <description>

&lt;p&gt;Scientists are increasingly dependent on computational techniques to
analyse large volumes of data.  These computational methods are often
tailored to the particular analysis in mind, and as such are valuable
research outputs.  Furthermore, unlike experimental techniques,
computational methods can be easily shared.  However, at least in
neuroscience, computational methods are not routinely shared upon
publication of associated manuscripts.&lt;/p&gt;

&lt;p&gt;To improve this situation, we have worked with the editors of &lt;em&gt;Nature
Neuroscience&lt;/em&gt; to establish a pilot &lt;em&gt;code review&lt;/em&gt; project.  Once papers
have been  approved in principle for publication, authors can opt-in to
the code review.  The code (and data) will be checked to see if
independent reviewers can reproduce key findings of the paper.  The
details of the code review process are outlined in the
&lt;a href=&#34;http://www.nature.com/neuro/journal/v20/n6/full/nn.4579.html&#34;&gt;editorial&lt;/a&gt;, and we have written a &lt;a href=&#34;http://www.nature.com/neuro/journal/v20/n6/full/nn.4550.html&#34;&gt;commentary&lt;/a&gt; to describe
good practice for sharing of code and data.  For example, we suggest
the minimum requirement for sharing is that sufficient code and data
be provided to regenerate a key figure/table of the paper.  This
follows the well-established requirements for submitting code to
&lt;a href=&#34;http://modeldb.yale.edu&#34;&gt;modeldb&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Although our current guidelines are focused around code and data in
Neuroscience, most of our suggestions apply across many scientific
disciplines.  Just as journals (and funders) now require the sharing
of data underlying a research paper, we believe the underlying code
should also be freely shared.  We hope that other journals
will adapt appropriate policies to allow for the long-term sharing and
reuse of scientific code.  By sharing the code and data relating to
research articles, communities will be able to reproduce and extend
upon each other&amp;rsquo;s work.  This should lead to more robust scientific
results and reduce duplication of effort.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;This project emerged from discussions at a workshop to encourage
sharing in neuroscience, held in Cambridge, December 2014.  It was
financially supported and organized by the &lt;a href=&#34;http://www.incf.org&#34;&gt;International
Neuroinformatics Coordinating Facility&lt;/a&gt;,
with additional support from the
&lt;a href=&#34;http://www.software.ac.uk&#34;&gt;Software Sustainability Institute&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;[1] Extending transparency to code. Nat Neurosci 20:761–761.
&lt;a href=&#34;http://www.nature.com/neuro/journal/v20/n6/full/nn.4579.html&#34;&gt;(Article)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] Eglen SJ, Marwick B, Halchenko YO, Hanke M, Sufi S, Gleeson P,
Angus Silver R, Davison AP, Lanyon L, Abrams M, Wachtler T, Willshaw
DJ, Pouzat C, Poline J-B (2017) Toward standard practices for sharing
computer code and programs in neuroscience. Nat Neurosci 20:770–773.  &lt;a href=&#34;http://www.nature.com/neuro/journal/v20/n6/full/nn.4550.html&#34;&gt;(Article)&lt;/a&gt;&lt;/p&gt;
</description>
          </item>
        
      
    
  </channel>
</rss>
